{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 102 Final Project: Election Finances\n",
    "\n",
    "By Jessie Houng, Rithika Neti, Ritvik Iyer, and Sunny Shen\n",
    "\n",
    "*Remark*: The Data Cleaning and EDA sections of the notebook are shown in reverse order compared to where they appear in the report because we performed EDA in multiple stages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required libraries \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install fuzzy-match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzy_match import match\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Settings\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Clean Endorsements Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Primary Candidates 2018 data\n",
    "dem = pd.read_csv('data/dem_candidates.csv', error_bad_lines=False, encoding='latin-1')\n",
    "rep = pd.read_csv('data/rep_candidates.csv', error_bad_lines=False, encoding='latin-1')\n",
    "\n",
    "# Remove candidates not running in national elections \n",
    "rep = rep[rep['Office Type'] != 'Governor']\n",
    "dem = dem[dem['Office Type'] != 'Governor']\n",
    "\n",
    "# Set column for party affiliations \n",
    "dem['Party'] = 'D'\n",
    "rep['Party'] = 'R'\n",
    "\n",
    "# Fill values for partisan lean for Republican candidates\n",
    "partisan_lean_by_district = dict(zip(dem['District'], dem['Partisan Lean']))\n",
    "rep['Partisan Lean'] = rep['District'].map(partisan_lean_by_district)\n",
    "prop_filled_v1 = len(rep['Partisan Lean'].dropna())/len(rep['Partisan Lean'])\n",
    "\n",
    "# Impute partisan lean for Republican candidates based on Senate, then average of House districts within state\n",
    "senate_leans = dem[dem['District'].str.contains('Senate')][['State', 'Partisan Lean']].drop_duplicates()\n",
    "partisan_lean_by_state = dict(zip(senate_leans['State'], senate_leans['Partisan Lean']))\n",
    "house_leans = dem[['Candidate','State','District','Partisan Lean']][\n",
    "    dem['District'].str.contains('House')].groupby('State').mean().reset_index().dropna()\n",
    "for state, lean in zip(house_leans['State'], house_leans['Partisan Lean']):\n",
    "    if state not in partisan_lean_by_state.keys():\n",
    "        partisan_lean_by_state[state] = lean\n",
    "for index, row in rep.iterrows():\n",
    "    state = rep.loc[index, 'State']\n",
    "    if (pd.isna(rep.loc[index, 'Partisan Lean'])) and (state in partisan_lean_by_state):\n",
    "        rep.loc[index, 'Partisan Lean'] = partisan_lean_by_state[state]\n",
    "prop_filled_v2 = len(rep['Partisan Lean'].dropna())/len(rep['Partisan Lean'])\n",
    "\n",
    "# Identify the states with Republican candidates who still have missing partisan leans \n",
    "missing_state_rep = rep[rep['Partisan Lean'].isna()]['State'].value_counts().index.values\n",
    "\n",
    "# Select the Republican candidates who have missing partisan leans \n",
    "invalid_rep_cand = rep[rep['State'].isin(missing_state_rep)]\n",
    "\n",
    "# Select the Republican candidates who do not have missing partisan leans \n",
    "valid_rep_cand = rep[~(rep['State'].isin(missing_state_rep))]\n",
    "\n",
    "# Deal with missing values in endorsement data\n",
    "final_dem = dem\n",
    "final_rep = valid_rep_cand\n",
    "dem_endorse_support_cols = [x for x in final_dem.columns if 'Support' in x or 'Endorsed' in x]\n",
    "rep_endorse_support_cols = [x for x in final_rep.columns if 'Support' in x or 'Endorsed' in x]\n",
    "endorse_map = {'Yes': 1, 'No': -1, np.nan: 0}\n",
    "final_dem.loc[:, dem_endorse_support_cols] = final_dem.loc[:, dem_endorse_support_cols].replace(endorse_map)\n",
    "final_rep.loc[:, rep_endorse_support_cols] = final_rep.loc[:, rep_endorse_support_cols].replace(endorse_map)\n",
    "party_support = np.hstack([final_dem['Party Support?'].values, final_rep['Rep Party Support?'].values])\n",
    "full_candidates = pd.concat([final_dem, final_rep]).drop(columns=['Party Support?', 'Rep Party Support?'])\n",
    "full_candidates['Party Support?'] = party_support\n",
    "cand_endorse_support_cols = [x for x in full_candidates.columns if 'Support' in x or 'Endorsed' in x]\n",
    "full_candidates.loc[:, cand_endorse_support_cols] = full_candidates.loc[:, cand_endorse_support_cols].fillna(-1)\n",
    "full_candidates = full_candidates.drop(columns=['General Status', 'Race', 'Veteran?',\n",
    "                                                'LGBTQ?', 'Elected Official?', 'Self-Funder?',\n",
    "                                               'STEM?', 'Obama Alum?', 'Guns Sense Candidate?',\n",
    "                                               'Won Primary'])\n",
    "\n",
    "# Set district numbers \n",
    "full_candidates['District_Num'] = [\n",
    "    int(x[-1]) if len(x[-1])<=2 else 0 for x in full_candidates['District'].str.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Clean FEC Campaigner Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in FEC All Candidates data\n",
    "fin16 = pd.read_csv('data/weball16.txt', sep=\"|\", header=None)\n",
    "fin18 = pd.read_csv('data/weball18.txt',  sep=\"|\", header=None)\n",
    "\n",
    "# Set column names  \n",
    "fin_col = ['CAND_ID', 'CAND_NAME', 'CAND_ICI', 'PTY_CD', 'CAND_PTY_AFFILIATION',\n",
    "           'TTL_RECEIPTS', 'TRANS_FROM_AUTH', 'TTL_DISB', 'TRANS_TO_AUTH', 'COH_BOP', \n",
    "           'COH_COP', 'CAND_CONTRIB', 'CAND_LOANS', 'OTHER_LOANS', 'CAND_LOAN_REPAY', \n",
    "           'OTHER_LOAN_REPAY', 'DEBTS_OWED_BY', 'TTL_INDIV_CONTRIB', 'CAND_OFFICE_ST', \n",
    "           'CAND_OFFICE_DISTRICT', 'SPEC_ELECTION', 'PRIM_ELECTION', 'RUN_ELECTION', \n",
    "           'GEN_ELECTION', 'GEN_ELECTION_PRECENT', 'OTHER_POL_CMTE_CONTRIB',\n",
    "           'POL_PTY_CONTRIB', 'CVG_END_DT', 'INDIV_REFUNDS', 'CMTE_REFUNDS']\n",
    "fin16.columns = fin_col\n",
    "fin18.columns = fin_col\n",
    "\n",
    "# Set year\n",
    "fin18['YEAR'] = 18\n",
    "fin16['YEAR'] = 16\n",
    "\n",
    "# Combine datasets \n",
    "fin = pd.concat([fin16, fin18])\n",
    "\n",
    "# Reformat party names \n",
    "fin['CAND_PTY_AFFILIATION'] = fin['CAND_PTY_AFFILIATION'].map({'REP':'R',\n",
    "                                                               'DEM':'D'})\n",
    "\n",
    "# Reformat candidate names  \n",
    "fin['CAND_NAME'] = [\" \".join(n.lower().title().split(\", \")[::-1]) for n in fin['CAND_NAME']]\n",
    "\n",
    "# Remove missing district numbers and re-code data \n",
    "fin = fin[~fin['CAND_OFFICE_DISTRICT'].isna()]\n",
    "fin['CAND_OFFICE_DISTRICT']= fin['CAND_OFFICE_DISTRICT'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Merge Endorsements Data with FEC Campaigner Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy match names \n",
    "fuzzy_name = [match.extract(x, fin['CAND_NAME'], limit=1, score_cutoff=0.55) for x in full_candidates['Candidate'].values]\n",
    "fuzzy_name_match = [x[0][0] if x!=[] else '' for x in fuzzy_name]\n",
    "fuzzy_name_match_score = [x[0][1] if x!=[] else '' for x in fuzzy_name]\n",
    "full_candidates['fuzzy_name_match'] = fuzzy_name_match\n",
    "full_candidates['fuzzy_name_match_score'] = fuzzy_name_match_score\n",
    "\n",
    "# Merge datasets based on fuzzy matched names, state, and party and drop duplicates running in special elections\n",
    "cand_fin = pd.merge(fin[['CAND_ID', 'CAND_NAME', 'CAND_OFFICE_DISTRICT', \n",
    "                         'CAND_OFFICE_ST', 'CAND_ICI', 'PTY_CD', \n",
    "                         'CAND_PTY_AFFILIATION', 'TTL_RECEIPTS', \n",
    "                        'TRANS_FROM_AUTH','CAND_CONTRIB',\n",
    "                        'TTL_INDIV_CONTRIB','OTHER_POL_CMTE_CONTRIB']], \n",
    "                    full_candidates, \n",
    "                    left_on=['CAND_NAME','CAND_OFFICE_ST', 'CAND_PTY_AFFILIATION'] , \n",
    "                    right_on=['fuzzy_name_match', 'State', 'Party'])\n",
    "cand_fin = cand_fin[['Candidate', 'CAND_ID', 'District', 'Party', 'State', 'Emily Endorsed?',\n",
    "                     'Partisan Lean', 'TTL_RECEIPTS', 'TRANS_FROM_AUTH', 'CAND_CONTRIB',\n",
    "                     'TTL_INDIV_CONTRIB', 'OTHER_POL_CMTE_CONTRIB', 'Primary %',\n",
    "                     'Biden Endorsed?', 'Warren Endorsed? ', 'Sanders Endorsed?',\n",
    "                     'Our Revolution Endorsed?', 'Justice Dems Endorsed?', \n",
    "                     'PCCC Endorsed?', 'Indivisible Endorsed?', 'WFP Endorsed?',\n",
    "                     'VoteVets Endorsed?', 'No Labels Support?', 'Party Support?', \n",
    "                     'Trump Endorsed?', 'Bannon Endorsed?', 'Great America Endorsed?',\n",
    "                     'NRA Endorsed?', 'Right to Life Endorsed?', 'Susan B. Anthony Endorsed?',\n",
    "                     'Club for Growth Endorsed?', 'Koch Support?', 'House Freedom Support?',\n",
    "                     'Tea Party Endorsed?', 'Main Street Endorsed?', 'Chamber Endorsed?', 'Race Type', 'CAND_ICI']]\n",
    "\n",
    "# Find duplicate candidates who are listed as running in multiple roles (i.e incumbent, open race, challenger)\n",
    "dupes = cand_fin.groupby('Candidate').count()[['CAND_ICI']].reset_index()\n",
    "dupe_names = dupes[dupes['CAND_ICI'] > 1]['Candidate'].values\n",
    "keep_dupe_idx = []\n",
    "for name in dupe_names:\n",
    "    filtered_dupe_cand = cand_fin[cand_fin['Candidate'] == name]\n",
    "    argmax_ind_cont = filtered_dupe_cand.TTL_INDIV_CONTRIB.argmax()\n",
    "    row_idx = filtered_dupe_cand.index[argmax_ind_cont]\n",
    "    keep_dupe_idx.append(row_idx)\n",
    "to_remove = cand_fin[cand_fin['Candidate'].isin(dupe_names)].drop(keep_dupe_idx).index.values\n",
    "cand_fin = cand_fin.drop(to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Clean FEC Individual Contributions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Individual Donor data\n",
    "indiv16 = pd.read_csv('data/indiv16/itcont.txt', sep=\"|\", header=None, low_memory=False)\n",
    "indiv18 = pd.read_csv('data/indiv18/itcont.txt', sep=\"|\", header=None, low_memory=False)\n",
    "indiv_cols = pd.read_csv('indiv_header_file.csv').columns\n",
    "\n",
    "# Filter down to campaign years we are interested in \n",
    "P2018_indiv16 = indiv16[indiv16[3]=='P2018']\n",
    "P2018_indiv18 = indiv18[indiv18[3]=='P2018']\n",
    "\n",
    "# Set column names \n",
    "P2018_indiv16.columns = indiv_cols\n",
    "P2018_indiv18.columns = indiv_cols\n",
    "\n",
    "# Concatenate data for 2018 primary cycle together\n",
    "P2018 = pd.concat([P2018_indiv16, P2018_indiv18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Clean FEC Campaign-Committee Linkages Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in linkage data\n",
    "ccl16 = pd.read_csv('data/ccl16.txt', sep='|', header=None)\n",
    "ccl18 = pd.read_csv('data/ccl18.txt', sep='|', header=None)\n",
    "\n",
    "# Set column names \n",
    "ccl_cols = ['CAND_ID', 'CAND_ELECTION_YR', 'FEC_ELECTION_YR', 'CMTE_ID', 'CMTE_TP', 'CMTE_DSGN', 'LINKAGE_ID']\n",
    "ccl16.columns = ccl_cols\n",
    "ccl18.columns = ccl_cols\n",
    "\n",
    "# Filter down to campaign years we are interested in \n",
    "ccl16 = ccl16[ccl16['CAND_ELECTION_YR']==2018]\n",
    "ccl18 = ccl18[ccl18['CAND_ELECTION_YR']==2018]\n",
    "\n",
    "# Concatenate linkages for 2018 primary cycle together \n",
    "ccl = pd.concat([ccl16, ccl18]).drop_duplicates(subset=['CAND_ID', 'CMTE_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Merge Individual Contributions with Campaign-Committee Linkages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Number of Unique individual contributions from 2018 contribution data \n",
    "cmt_count = P2018[['CMTE_ID', 'TRANSACTION_AMT', 'NAME']].groupby(\n",
    "    ['CMTE_ID', 'NAME']).count().reset_index().groupby('CMTE_ID').count().reset_index()\n",
    "cmt_count = cmt_count.drop(columns=['NAME']).rename(columns={'TRANSACTION_AMT': 'Num_Unique_Contributions'})\n",
    "\n",
    "# Merge contributions by committee id to linkage data\n",
    "cand_count = pd.merge(cmt_count, ccl, on = 'CMTE_ID').drop(columns=['CMTE_TP', 'CMTE_DSGN', 'LINKAGE_ID', \n",
    "                                                      'CAND_ELECTION_YR', 'FEC_ELECTION_YR']).groupby(\n",
    "                                                    'CAND_ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Merge Candidate Endorsements with Number of Unique Donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Method #1: Multiple Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Method #2: Bayesian Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
